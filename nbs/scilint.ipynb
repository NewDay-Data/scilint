{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: infuse quality into notebook based workflows with a new type of build tool.\n",
    "output-file: scilint.html\n",
    "title: Data Science Notebook Linting\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | include: false\n",
    "# | default_exp scilint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from configparser import InterpolationMissingOptionError\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Iterable, Tuple\n",
    "\n",
    "import nbformat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from execnb.nbio import read_nb\n",
    "from fastcore.script import Param, call_parse, store_false\n",
    "from fastcore.xtras import globtastic\n",
    "from nbdev.clean import nbdev_clean\n",
    "from nbdev.config import get_config\n",
    "from nbdev.doclinks import nbdev_export, nbglob\n",
    "from nbdev.quarto import nbdev_docs, nbdev_readme\n",
    "from nbdev.test import nbdev_test\n",
    "from nbqa.__main__ import _get_configs, _main\n",
    "from nbqa.cmdline import CLIArgs\n",
    "from nbqa.find_root import find_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbdev_path = Path(Path(\".\").resolve(), \"example_nbs\", \"nbdev.ipynb\")\n",
    "nbdev_hq_path = Path(Path(\".\").resolve(), \"example_nbs\", \"nbdev_high_quality.ipynb\")\n",
    "non_nbdev_path = Path(Path(\".\").resolve(), \"example_nbs\", \"non_nbdev.ipynb\")\n",
    "non_nbdev_lq_path = Path(\n",
    "    Path(\".\").resolve(), \"example_nbs\", \"non_nbdev_low_quality.ipynb\"\n",
    ")\n",
    "index_path = Path(Path(\".\").resolve(), \"index.ipynb\")\n",
    "syntax_error_path = Path(Path(\".\").resolve(), \"syntax_error.ipynb\")\n",
    "\n",
    "nbdev_nb = read_nb(nbdev_path)\n",
    "nbdev_hq_nb = read_nb(nbdev_hq_path)\n",
    "non_nbdev_nb = read_nb(non_nbdev_path)\n",
    "non_nbdev_lq_nb = read_nb(non_nbdev_lq_path)\n",
    "index = read_nb(index_path)\n",
    "syntax_error = read_nb(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `scilint_tidy` \n",
    "\n",
    "simple wrapper around no-decisions version of nbqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def run_nbqa_cmd(cmd):\n",
    "    print(f\"Running {cmd}\")\n",
    "    project_root: Path = find_project_root(tuple([str(Path(\".\").resolve())]))\n",
    "    args = CLIArgs.parse_args([cmd, str(project_root)])\n",
    "    configs = _get_configs(args, project_root)\n",
    "    output_code = _main(args, configs)\n",
    "    return output_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_root: Path = find_project_root(tuple([str(Path(\".\").resolve())]))\n",
    "assert os.path.basename(project_root) == \"scilint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def tidy():\n",
    "    tidy_tools = [\"black\", \"isort\", \"autoflake\"]\n",
    "    [run_nbqa_cmd(c) for c in tidy_tools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_project_root(path: Path = Path(\".\").resolve()):\n",
    "    return find_project_root(tuple([str()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def is_nbdev_project(project_path: Path = Path(\".\")):\n",
    "    is_nbdev = True\n",
    "    project_root = find_project_root(tuple([str(project_path.resolve())]))\n",
    "\n",
    "    if not Path(project_root, \"settings.ini\").exists():\n",
    "        is_nbdev = False\n",
    "    try:\n",
    "        get_config().lib_name\n",
    "    except InterpolationMissingOptionError:\n",
    "        is_nbdev = False\n",
    "\n",
    "    return is_nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert is_nbdev_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    assert not is_nbdev_project(Path(tmp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_func_defs(code, ignore_private_prefix=True):\n",
    "    func_names = []\n",
    "    for stmt in ast.walk(ast.parse(code)):\n",
    "        if isinstance(stmt, ast.FunctionDef):\n",
    "            inner_cond = (\n",
    "                False if ignore_private_prefix and stmt.name.startswith(\"_\") else True\n",
    "            )\n",
    "            if inner_cond:\n",
    "                func_names.append(stmt.name)\n",
    "    return func_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_code = \"\"\"\n",
    "x()\n",
    "def y():\n",
    "    pass\n",
    "def z():\n",
    "    def a():\n",
    "        pass\n",
    "class A():\n",
    "    def b():\n",
    "        pass\n",
    "def blabla():\n",
    "    return 1\n",
    "def _hidden():\n",
    "    return None\n",
    "\"\"\"\n",
    "func_defs = [\"a\", \"b\", \"blabla\", \"y\", \"z\"]\n",
    "assert func_defs == sorted(get_func_defs(test_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def count_func_calls(code, func_defs):\n",
    "    func_calls = Counter({k: 0 for k in func_defs})\n",
    "    for stmt in ast.walk(ast.parse(code)):\n",
    "        if isinstance(stmt, ast.Call):\n",
    "            if type(stmt.func) == ast.Subscript:\n",
    "                func_name = stmt.func.value.id\n",
    "            else:\n",
    "                func_name = (\n",
    "                    stmt.func.id if \"id\" in stmt.func.__dict__ else stmt.func.attr\n",
    "                )\n",
    "            if func_name in func_defs:\n",
    "                if func_name in func_calls:\n",
    "                    func_calls[func_name] += 1\n",
    "    return func_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_code = \"\"\"self.hierarchical_topic_reduction(3); \n",
    "topic_reduction(3); \n",
    "lambda x: topic(x); \n",
    "hierarchical_topic_reduction[4]; \n",
    "hierarchical_topic_reduction(4); \n",
    "blabla()\n",
    "lambda y: other(5)\n",
    "funcs = [x, y]\n",
    "funcs[0](3)\n",
    "\"\"\"\n",
    "test_func_defs = [\n",
    "    \"topic\",\n",
    "    \"topic_reduction\",\n",
    "    \"blablabla\",\n",
    "    \"hierarchical_topic_reduction\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert count_func_calls(test_code, test_func_defs) == Counter(\n",
    "    {\n",
    "        \"topic\": 1,\n",
    "        \"topic_reduction\": 1,\n",
    "        \"blablabla\": 0,\n",
    "        \"hierarchical_topic_reduction\": 2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_cell_code = r\"\"\"\n",
    "def something():\n",
    "    pass; pass # in x 2\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!ls -l\n",
    "if 1!= 2:\n",
    "    print(4)\n",
    "#| export\n",
    "\n",
    "import pandas as pd # out\n",
    "from sciflow.utils import lib_path, odbc_connect, query # out\n",
    "\n",
    "#| export\n",
    "\n",
    "def nb_to_sagemaker_pipeline(\n",
    "    nb_path: Path,\n",
    "    silent: bool = True,\n",
    "):\n",
    "    nb = read_nb(nb_path)  # in\n",
    "    lib_name = get_config().get(\"lib_name\")  # in\n",
    "    module_name = find_default_export(nb[\"cells\"])  # in\n",
    "    \n",
    "x = [1,2,3] # out\n",
    "nb_to_sagemaker_pipeline() # out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def replace_ipython_magics(code):\n",
    "    # Replace Ipython magic and shell command symbol with comment\n",
    "    code = code.replace(\"%\", \"#\")\n",
    "    code = re.sub(r\"^!\", \"#\", code)\n",
    "    return re.sub(r\"\\n\\W?!\", \"\\n#\", code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "throws = False\n",
    "try:\n",
    "    assert ast.parse(nb_cell_code)\n",
    "except SyntaxError:\n",
    "    throws = True\n",
    "assert throws\n",
    "assert type(ast.parse(replace_ipython_magics(nb_cell_code))) == ast.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def safe_div(numer, denom):\n",
    "    return 0 if denom == 0 else numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert safe_div(1, 1) == 1\n",
    "assert safe_div(2, 1) == 2\n",
    "assert safe_div(1, 2) == 0.5\n",
    "assert safe_div(0, 1) == 0\n",
    "assert safe_div(1, 0) == 0\n",
    "assert safe_div(10, 1) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_cell_code(nb):\n",
    "    pnb = nbformat.from_dict(nb)\n",
    "    nb_cell_code = \"\\n\".join(\n",
    "        [\n",
    "            replace_ipython_magics(c[\"source\"])\n",
    "            for c in pnb.cells\n",
    "            if c[\"cell_type\"] == \"code\"\n",
    "        ]\n",
    "    )\n",
    "    return nb_cell_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Potential Quality Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Calls-per-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def calls_per_func(nb):\n",
    "    nb_cell_code = get_cell_code(nb)\n",
    "    func_defs = get_func_defs(nb_cell_code)\n",
    "    func_calls = count_func_calls(nb_cell_code, func_defs)\n",
    "    return func_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def mean_cpf(nb):\n",
    "    return pd.Series(calls_per_func(nb)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def median_cpf(nb):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action=\"ignore\", message=\"Mean of empty slice\")\n",
    "        return pd.Series(calls_per_func(nb)).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert mean_cpf(nbdev_nb).round(2) == 2.23\n",
    "assert median_cpf(nbdev_nb) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert mean_cpf(read_nb(nbdev_path)).round(2) == 2.23\n",
    "assert mean_cpf(read_nb(nbdev_hq_path)).round(2) == 2.5\n",
    "assert mean_cpf(read_nb(non_nbdev_path)).round(2) == 1.0\n",
    "assert mean_cpf(read_nb(non_nbdev_lq_path)).round(2) == 1.62\n",
    "assert pd.isnull(mean_cpf(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert median_cpf(read_nb(nbdev_path)) == 1.0\n",
    "assert median_cpf(read_nb(nbdev_hq_path)).round(2) == 1.5\n",
    "assert median_cpf(read_nb(non_nbdev_path)).round(2) == 1.0\n",
    "assert median_cpf(read_nb(non_nbdev_lq_path)).round(2) == 1.0\n",
    "assert pd.isnull(median_cpf(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Asserts-to-Function Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asserted_code = r\"\"\"\n",
    "def something():\n",
    "    pass; pass # in x 2\n",
    "    \n",
    "assert True\n",
    "\n",
    "#| export\n",
    "\n",
    "def nb_to_sagemaker_pipeline(\n",
    "    nb_path: Path,\n",
    "    silent: bool = True,\n",
    "):\n",
    "    nb = read_nb(nb_path)  # in\n",
    "    lib_name = get_config().get(\"lib_name\")  # in\n",
    "    module_name = find_default_export(nb[\"cells\"])  # in\n",
    "    \n",
    "x = [1,2,3] # out\n",
    "assert len(x) > 2\n",
    "assert something() is None # something +1\n",
    "\n",
    "def tr():\n",
    "    return True\n",
    "    \n",
    "def get_seg(num):\n",
    "    return 2\n",
    "    \n",
    "assert(tr)\n",
    "assert(tr()) # tr +1\n",
    "assert(tr() == 4) # tr +1\n",
    "assert(4 ==tr()) # tr +1\n",
    "assert 0 != 0\n",
    "assert \"' '\".join(tr(1)) == \"00\" # tr +1\n",
    "assert len(get_seg(50)) == 50 # get_seg +1\n",
    "assert max([int(x) for x in get_seg(100)]) == 99 # get_seg +1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat as nbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asserted_nb = nbf.v4.new_notebook()\n",
    "asserted_nb[\"cells\"] = [nbf.v4.new_code_cell(asserted_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def afr(nb):\n",
    "    nb_cell_code = get_cell_code(nb)\n",
    "    if nb_cell_code == \"\":  # no code cells - metric is not well defined\n",
    "        return np.nan\n",
    "    func_defs = get_func_defs(nb_cell_code)\n",
    "    num_funcs = len(func_defs)\n",
    "\n",
    "    assert_count = 0\n",
    "    for stmt in ast.walk(ast.parse(nb_cell_code)):\n",
    "        if isinstance(stmt, ast.Assert):\n",
    "            assert_count += 1\n",
    "\n",
    "    return safe_div(assert_count, num_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert afr(nbdev_nb) > 1\n",
    "assert afr(nbdev_hq_nb) > 1\n",
    "assert afr(non_nbdev_nb) == 0\n",
    "assert afr(non_nbdev_lq_nb) == 0\n",
    "assert pd.isnull(afr(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. In-line Asserts Per Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def count_inline_asserts(code, func_defs):\n",
    "    inline_func_asserts = Counter({k: 0 for k in func_defs})\n",
    "\n",
    "    for stmt in ast.walk(ast.parse(code)):\n",
    "        if isinstance(stmt, ast.Assert):\n",
    "            for assert_st in ast.walk(stmt):\n",
    "                if isinstance(assert_st, ast.Call):\n",
    "                    func_name = (\n",
    "                        assert_st.func.id\n",
    "                        if \"id\" in assert_st.func.__dict__\n",
    "                        else assert_st.func.attr\n",
    "                    )\n",
    "                    if func_name in inline_func_asserts:\n",
    "                        inline_func_asserts[func_name] += 1\n",
    "    return inline_func_asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def iaf(nb):\n",
    "    nb_cell_code = get_cell_code(nb)\n",
    "    if nb_cell_code == \"\":\n",
    "        return np.nan\n",
    "    func_defs = get_func_defs(nb_cell_code)\n",
    "    return count_inline_asserts(nb_cell_code, func_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func_defs = get_func_defs(asserted_code)\n",
    "inline_asserts_expected = Counter(\n",
    "    {\"something\": 1, \"tr\": 4, \"get_seg\": 2, \"nb_to_sagemaker_pipeline\": 0}\n",
    ")\n",
    "inline_asserts_actual = count_inline_asserts(asserted_code, func_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert inline_asserts_actual == inline_asserts_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 0.0 == pd.Series(iaf(nbdev_nb)).median()\n",
    "assert 0.0 == pd.Series(iaf(nbdev_hq_nb)).median()\n",
    "assert 0.0 == pd.Series(iaf(non_nbdev_nb)).median()\n",
    "assert 0.0 == pd.Series(iaf(non_nbdev_lq_nb)).median()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(action=\"ignore\", message=\"Mean of empty slice\")\n",
    "    assert pd.isnull(pd.Series(iaf(index)).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'scalar': 0, 'py_advanced': 0, 'pandas': 0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iaf(non_nbdev_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'get_traffic_text': 0,\n",
       "         'get_experiment_segment': 0,\n",
       "         'evaluate': 0,\n",
       "         'serve_num_topics': 0,\n",
       "         'get_num_topics': 0,\n",
       "         'get_topic_sizes': 0,\n",
       "         'get_topics': 0,\n",
       "         'plot_wordcloud': 0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iaf(non_nbdev_lq_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert inline_asserts_expected == iaf(asserted_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def mean_iaf(nb):\n",
    "    return pd.Series(iaf(nb)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def median_iaf(nb):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action=\"ignore\", message=\"Mean of empty slice\")\n",
    "        return pd.Series(iaf(nb)).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What about calculating functions code coverage\n",
    "* How does pytest-cov do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. In-function Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def calc_ifp(nb_cell_code):\n",
    "    stmts_in_func = 0\n",
    "    stmts_outside_func = 0\n",
    "    for stmt in ast.walk(ast.parse(replace_ipython_magics(nb_cell_code))):\n",
    "        if isinstance(stmt, ast.FunctionDef) and not stmt.name.startswith(\"_\"):\n",
    "            for body_item in stmt.body:\n",
    "                stmts_in_func += 1\n",
    "        elif isinstance(stmt, ast.Module):\n",
    "            for body_item in stmt.body:\n",
    "                if not isinstance(body_item, ast.FunctionDef):\n",
    "                    stmts_outside_func += 1\n",
    "    return (\n",
    "        0\n",
    "        if stmts_outside_func + stmts_in_func == 0\n",
    "        else (stmts_in_func / (stmts_outside_func + stmts_in_func)) * 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert (calc_ifp(nb_cell_code)) == (5 / (5 + 5)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def ifp(nb):\n",
    "    nb_cell_code = \"\\n\".join(\n",
    "        [\n",
    "            replace_ipython_magics(c[\"source\"])\n",
    "            for c in nb.cells\n",
    "            if c[\"cell_type\"] == \"code\"\n",
    "        ]\n",
    "    )\n",
    "    if nb_cell_code == \"\":\n",
    "        return np.nan\n",
    "    return calc_ifp(nb_cell_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert ifp(nbdev_nb) >= 0\n",
    "assert ifp(nbdev_hq_nb) >= 0\n",
    "assert ifp(non_nbdev_nb) >= 0\n",
    "assert ifp(non_nbdev_lq_nb) >= 0\n",
    "assert pd.isnull(ifp(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Markdown to Code Percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def mcp(nb):\n",
    "    md_cells = [c for c in nb.cells if c[\"cell_type\"] == \"markdown\"]\n",
    "    code_cells = [c for c in nb.cells if c[\"cell_type\"] == \"code\"]\n",
    "    num_code_cells = len(code_cells)\n",
    "    if num_code_cells == 0:\n",
    "        return np.nan\n",
    "    num_md_cells = len(md_cells)\n",
    "    return (\n",
    "        100\n",
    "        if num_code_cells == 0\n",
    "        else (num_md_cells / (num_md_cells + num_code_cells)) * 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert mcp(nbdev_nb) >= 0\n",
    "assert mcp(nbdev_hq_nb) >= 0\n",
    "assert mcp(non_nbdev_nb) >= 0\n",
    "assert mcp(non_nbdev_lq_nb) >= 0\n",
    "assert pd.isnull(mcp(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Total Code Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def tcl(nb):\n",
    "    return sum([len(c[\"source\"]) for c in nb.cells if c[\"cell_type\"] == \"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert tcl(nbdev_nb) >= 50\n",
    "assert tcl(nbdev_hq_nb) >= 50\n",
    "assert tcl(non_nbdev_nb) >= 50\n",
    "assert tcl(non_nbdev_lq_nb) >= 50\n",
    "assert tcl(index) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lines-of-code per Markdown Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def loc_per_md_section(nb):\n",
    "    num_md_sections = len(\n",
    "        [\n",
    "            c[\"source\"]\n",
    "            for c in nb.cells\n",
    "            if c[\"cell_type\"] == \"markdown\" and c[\"source\"].strip().startswith(\"#\")\n",
    "        ]\n",
    "    )\n",
    "    total_code_len = tcl(nb)\n",
    "    if total_code_len == 0 or num_md_sections == 0:\n",
    "        result = np.nan\n",
    "    else:\n",
    "        result = tcl(nb) / num_md_sections\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert loc_per_md_section(nbdev_nb) < 1000\n",
    "assert loc_per_md_section(nbdev_hq_nb) < 1000\n",
    "assert loc_per_md_section(non_nbdev_nb) is np.nan\n",
    "assert loc_per_md_section(non_nbdev_lq_nb) > 1000\n",
    "assert loc_per_md_section(index) is np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quality Indicator Function Map\n",
    "\n",
    "> Add new quality indicators here to be used. Signature contract is nb -> number. TODO: provide a proper typed signature, handle bools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "indicator_funcs = {\n",
    "    \"calls_per_func_mean\": mean_cpf,\n",
    "    \"calls_per_func_median\": median_cpf,\n",
    "    \"asserts_func_ratio\": afr,\n",
    "    \"inline_asserts_per_func_mean\": mean_iaf,\n",
    "    \"inline_asserts_per_func_median\": median_iaf,\n",
    "    \"in_func_pct\": ifp,\n",
    "    \"markdown_code_pct\": mcp,\n",
    "    \"loc_per_md_section\": loc_per_md_section,\n",
    "    \"total_code_len\": tcl,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `lint_nb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def lint_nb(\n",
    "    spec_name: str,\n",
    "    nb_path: Path,\n",
    "    conf: Dict[str, Any],\n",
    "    indicators: Dict[str, Callable],\n",
    "    include_in_scoring: bool,\n",
    ") -> Tuple[float]:\n",
    "    nb = read_nb(nb_path)\n",
    "\n",
    "    has_syntax_error = False\n",
    "    indic_vals = list(np.repeat(np.nan, len(indicators)))\n",
    "    try:\n",
    "        for i, indic_name in enumerate(indicators):\n",
    "            indic_vals[i] = round(indicators[indic_name](nb), conf[\"precision\"])\n",
    "    except SyntaxError as se:\n",
    "        if conf[\"print_syntax_errors\"]:\n",
    "            print(f\"Syntax error in notebook: {nb_path} reason: \", se)\n",
    "        has_syntax_error = True\n",
    "    indic_vals.append(has_syntax_error)\n",
    "    indic_vals.append(include_in_scoring)\n",
    "    indic_vals.insert(0, spec_name)\n",
    "\n",
    "    return tuple(indic_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `_get_excluded_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_excluded_paths(paths: Iterable[Path], exclude_pattern: str) -> Iterable[Path]:\n",
    "    \"\"\"Excluded paths should either be absolute paths or paths rooted at the project root directory\"\"\"\n",
    "    excl_paths = []\n",
    "    paths = [p.absolute() for p in paths]\n",
    "\n",
    "    for ex_pattern in exclude_pattern.split(\",\"):\n",
    "        if Path(ex_pattern).is_absolute():\n",
    "            ex_path = Path(ex_pattern)\n",
    "        else:\n",
    "            ex_path = Path(get_project_root(), ex_pattern)\n",
    "\n",
    "        if ex_path.exists():\n",
    "            excl_paths.extend([p for p in paths if ex_pattern in str(p)])\n",
    "        elif not ex_path.exists():\n",
    "            raise ValueError(f\"Path component: {ex_path} does not exist\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid exclusion pattern: {ex_path} pattern is comma separrated list of 'dir/' for directories and 'name.ipynb' for specific notebook\"\n",
    "            )\n",
    "    return excl_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = [Path(p) for p in nbglob(Path(\".\"))]\n",
    "assert sorted(\n",
    "    [\n",
    "        p.name\n",
    "        for p in _get_excluded_paths(\n",
    "            paths, exclude_pattern=\"nbs/example_nbs/experimental,nbs/index.ipynb\"\n",
    "        )\n",
    "    ]\n",
    ") == sorted([\"non_nbdev.ipynb\", \"nbdev.ipynb\", \"index.ipynb\"])\n",
    "assert sorted(\n",
    "    [\n",
    "        p.name\n",
    "        for p in _get_excluded_paths(\n",
    "            paths, exclude_pattern=\"nbs/example_nbs/nbdev.ipynb\"\n",
    "        )\n",
    "    ]\n",
    ") == sorted([\"nbdev.ipynb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_calculate_warnings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _calculate_warnings(\n",
    "    spec_name: str,\n",
    "    scoring_report: pd.DataFrame,\n",
    "    conf: Dict[str, Any],\n",
    "    include_missing: bool = False,\n",
    ") -> Tuple[Dict[str, Any], int]:\n",
    "    warning_details = []\n",
    "\n",
    "    for op_text in list(conf[\"warnings\"].keys()):\n",
    "        for indic in conf[\"warnings\"][op_text]:\n",
    "            metric_series = scoring_report[indic]\n",
    "            or_exp = pd.isnull(metric_series) if include_missing else False\n",
    "            op = (\n",
    "                operator.lt\n",
    "                if op_text == \"lt\"\n",
    "                else operator.gt\n",
    "                if op_text == \"gt\"\n",
    "                else operator.eq\n",
    "            )\n",
    "            warning_data = metric_series[\n",
    "                (op(metric_series, conf[\"warnings\"][op_text][indic])) | (or_exp)\n",
    "            ]\n",
    "            warning_dict = warning_data.to_dict()\n",
    "            for key, val in warning_dict.items():\n",
    "                warning_dict[key] = (\n",
    "                    indic,\n",
    "                    val,\n",
    "                    op_text,\n",
    "                    conf[\"warnings\"][op_text][indic],\n",
    "                )\n",
    "            warning_details.append(warning_dict)\n",
    "\n",
    "    all_warns = _reshape_warnings(spec_name, scoring_report, warning_details)\n",
    "    num_warnings = len(all_warns)\n",
    "    return all_warns, num_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_reshape_warnings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _reshape_warnings(\n",
    "    spec_name: str, scoring_report: pd.DataFrame, warning_details: Iterable[Any]\n",
    ") -> Dict[str, Iterable[Tuple]]:\n",
    "    warnings_by_nb = {nb: [] for nb in scoring_report.index}\n",
    "    for nb in scoring_report.index:\n",
    "        for wd in warning_details:\n",
    "            if nb in wd:\n",
    "                warnings_by_nb[nb].append(tuple([spec_name, nb] + list(wd[nb])))\n",
    "    warnings_by_nb = {key: val for key, val in warnings_by_nb.items() if len(val) > 0}\n",
    "    flattened_warns = [item for sublist in warnings_by_nb.values() for item in sublist]\n",
    "    return pd.DataFrame.from_records(\n",
    "        data=flattened_warns,\n",
    "        columns=[\n",
    "            \"spec_name\",\n",
    "            \"notebook\",\n",
    "            \"indicator\",\n",
    "            \"value\",\n",
    "            \"operator\",\n",
    "            \"threshold\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lint_nbs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def lint_nbs(\n",
    "    spec_name: str,\n",
    "    conf: Dict[str, Any],\n",
    "    indicators: Dict[str, Callable],\n",
    "    nb_paths: Iterable[Path] = None,\n",
    "    nb_glob: Path = None,\n",
    "):\n",
    "    if nb_paths is None:\n",
    "        nb_paths = [Path(p).absolute() for p in nbglob(nb_glob)]\n",
    "    else:\n",
    "        nb_paths = [Path(p).absolute() for p in nb_paths]\n",
    "\n",
    "    if len(nb_paths) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    excluded_paths = None\n",
    "    exclusions = conf[\"exclusions\"]\n",
    "    if exclusions is not None:\n",
    "        excluded_paths = _get_excluded_paths(nb_paths, exclude_pattern=exclusions)\n",
    "\n",
    "    results = []\n",
    "    nb_names = []\n",
    "    for nb_path in nb_paths:\n",
    "        include_in_scoring = True\n",
    "        if exclusions is not None:\n",
    "            include_in_scoring = False if nb_path in excluded_paths else True\n",
    "\n",
    "        nb_names.append(nb_path.stem)\n",
    "        lint_result = lint_nb(spec_name, nb_path, conf, indicators, include_in_scoring)\n",
    "        results.append(lint_result)\n",
    "\n",
    "    lint_report = pd.DataFrame.from_records(\n",
    "        data=results,\n",
    "        index=nb_names,\n",
    "        columns=[\"spec_name\"]\n",
    "        + list(indicators.keys())\n",
    "        + [\"has_syntax_error\", \"include_in_scoring\"],\n",
    "    ).sort_values([\"in_func_pct\", \"markdown_code_pct\"], ascending=False)\n",
    "\n",
    "    scoring_report = lint_report[lint_report.include_in_scoring].copy()\n",
    "    all_warns, num_warnings = _calculate_warnings(spec_name, scoring_report, conf)\n",
    "    return lint_report, all_warns, num_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_map_paths_to_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _map_paths_specs(nb_glob: Path = None, specs_glob: Path = Path(\".\").resolve()):\n",
    "    nbs = nbglob(nb_glob)\n",
    "    spec_files = [\n",
    "        Path(p)\n",
    "        for p in globtastic(\n",
    "            specs_glob,\n",
    "            file_glob=\"scilint-*.yaml\",\n",
    "            skip_folder_re=\"ipynb_checkpoints|_proc\",\n",
    "        )\n",
    "    ]\n",
    "    [p.name for p in spec_files]\n",
    "    default_spec_files = [p for p in spec_files if p.name == \"scilint-default.yaml\"]\n",
    "    default_spec_file = default_spec_files[0] if len(default_spec_files) > 0 else None\n",
    "    spec_dirs = [p.parent for p in spec_files]\n",
    "\n",
    "    spec_nbs = {k: [] for k in spec_files}\n",
    "    for nb in [Path(p) for p in nbs]:\n",
    "        found_spec = False\n",
    "        for name, spec_dir in zip(spec_files, spec_dirs):\n",
    "            if nb.parent == spec_dir:\n",
    "                spec_nbs[name].append(nb)\n",
    "                found_spec = True\n",
    "        if not found_spec:\n",
    "            if default_spec_file is not None:\n",
    "                spec_nbs[default_spec_file].append(nb)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"No spec found for: {nb} and no default spec provided; \\\n",
    "                add custom spec for dir or create scilint-default.yaml spec\"\n",
    "                )\n",
    "\n",
    "    return spec_nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raises = False\n",
    "try:\n",
    "    _map_paths_specs(None, Path(Path(\".\").resolve(), \"example_nbs\"))\n",
    "except ValueError:\n",
    "    raises = True\n",
    "assert raises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legacy_dir = Path(Path(\".\").resolve(), \"example_nbs/legacy\")\n",
    "legacy_spec_nbs = _map_paths_specs(legacy_dir, legacy_dir)\n",
    "legacy_spec = Path(Path(\".\").resolve(), \"example_nbs\", \"legacy\", \"scilint-legacy.yaml\")\n",
    "assert legacy_spec in legacy_spec_nbs\n",
    "assert len(legacy_spec_nbs[legacy_spec]) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legacy_dir = Path(Path(\".\").resolve(), \"example_nbs/legacy\")\n",
    "legacy_spec_nbs = _map_paths_specs(legacy_dir)\n",
    "legacy_spec = Path(Path(\".\").resolve(), \"example_nbs\", \"legacy\", \"scilint-legacy.yaml\")\n",
    "assert legacy_spec in legacy_spec_nbs\n",
    "assert len(legacy_spec_nbs[legacy_spec]) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_nbs = _map_paths_specs()\n",
    "assert len(spec_nbs[Path(Path(\".\").resolve(), \"scilint-default.yaml\")]) == 7\n",
    "assert (\n",
    "    len(\n",
    "        spec_nbs[\n",
    "            Path(\n",
    "                Path(\".\").resolve(),\n",
    "                \"example_nbs\",\n",
    "                \"exploratory\",\n",
    "                f\"scilint-exploratory.yaml\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    == 3\n",
    ")\n",
    "assert (\n",
    "    len(\n",
    "        spec_nbs[\n",
    "            Path(\n",
    "                Path(\".\").resolve(),\n",
    "                \"example_nbs\",\n",
    "                \"experimental\",\n",
    "                f\"scilint-experimental.yaml\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    == 2\n",
    ")\n",
    "assert (\n",
    "    len(\n",
    "        spec_nbs[\n",
    "            Path(\n",
    "                Path(\".\").resolve(),\n",
    "                \"example_nbs\",\n",
    "                \"validated\",\n",
    "                f\"scilint-validated.yaml\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    == 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_nbs = _map_paths_specs(specs_glob=get_project_root())\n",
    "assert sorted([k.name for k in spec_nbs.keys()]) == sorted(\n",
    "    [\n",
    "        \"scilint-default.yaml\",\n",
    "        \"scilint-validated.yaml\",\n",
    "        \"scilint-experimental.yaml\",\n",
    "        \"scilint-exploratory.yaml\",\n",
    "        \"scilint-legacy.yaml\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `lint_nbs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = yaml.safe_load(Path(\"scilint-default.yaml\").read_text())\n",
    "default_spec_paths = list(_map_paths_specs().values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lint_report, all_warns, num_warns = lint_nbs(\n",
    "    \"scilint-default.yaml\", conf, indicator_funcs, nb_paths=default_spec_paths\n",
    ")\n",
    "assert num_warns == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf[\"exclusions\"] = None\n",
    "lint_report, all_warns, num_warns = lint_nbs(\n",
    "    \"scilint-default.yaml\", conf, indicator_funcs, nb_paths=default_spec_paths\n",
    ")\n",
    "assert num_warns == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = yaml.safe_load(\n",
    "    Path(\n",
    "        Path(\".\").resolve(), \"example_nbs\", \"experimental\", \"scilint-experimental.yaml\"\n",
    "    ).read_text()\n",
    ")\n",
    "lint_report, all_warns, num_warns = lint_nbs(\n",
    "    \"scilint-experimental.yaml\",\n",
    "    conf,\n",
    "    indicator_funcs,\n",
    "    nb_glob=Path(\"example_nbs/experimental/\"),\n",
    ")\n",
    "assert num_warns == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf[\"exclusions\"] = \"\"\"nbs/example_nbs/,nbs/index.ipynb\"\"\"\n",
    "_, all_warns, num_warns = lint_nbs(\n",
    "    \"scilint-experimental.yaml\",\n",
    "    conf,\n",
    "    indicator_funcs,\n",
    "    nb_glob=Path(\"example_nbs/experimental/\"),\n",
    ")\n",
    "assert num_warns == 0\n",
    "conf[\"exclusions\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf[\"exclusions\"] = \"\"\"nbs/example_nbs/experimental/non_nbdev.ipynb\"\"\"\n",
    "_, all_warns, num_warns = lint_nbs(\n",
    "    \"scilint-experimental.yaml\",\n",
    "    conf,\n",
    "    indicator_funcs,\n",
    "    nb_glob=Path(\"example_nbs/experimental/\"),\n",
    ")\n",
    "assert num_warns == 0\n",
    "conf[\"exclusions\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf[\"exclusions\"] = \"\"\"nbs/example_nbs/exploratory/syntax_error.ipynb\"\"\"\n",
    "_, all_warns, num_warns = lint_nbs(\n",
    "    \"scilint-exploratory.yaml\",\n",
    "    conf,\n",
    "    indicator_funcs,\n",
    "    nb_glob=Path(\"example_nbs/exploratory/\"),\n",
    ")\n",
    "assert num_warns == 3\n",
    "conf[\"exclusions\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `display_warning_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def display_warning_report(all_warns: pd.DataFrame):\n",
    "    print(\n",
    "        \"\\n******************************************Begin Scilint Warning Report*****************************************\"\n",
    "    )\n",
    "    print(all_warns.to_markdown(tablefmt=\"grid\", index=False))\n",
    "    print(\n",
    "        \"\\n******************************************End Scilint Warning Report*******************************************\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_persist_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _persist_results(\n",
    "    lint_report: pd.DataFrame, all_warns: pd.DataFrame, conf: Dict[str, Any]\n",
    "):\n",
    "    out_dir = Path(conf[\"out_dir\"])\n",
    "    conf_to_persist = {k: v for k, v in conf.items() if k != \"indicators\"}\n",
    "    if not out_dir.exists():\n",
    "        Path(out_dir).mkdir()\n",
    "    with open(Path(out_dir, \"scilint_config.json\"), \"w\") as outfile:\n",
    "        json.dump(conf_to_persist, outfile)\n",
    "    all_warns.to_csv(Path(out_dir, \"scilint_warnings.csv\"), index=False)\n",
    "    lint_report.to_csv(Path(out_dir, \"scilint_report.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    report = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "    _persist_results(report, report, {\"indicators\": [], \"out_dir\": tmp_dir})\n",
    "    assert pd.read_csv(Path(tmp_dir, \"scilint_report.csv\"), index_col=0).equals(\n",
    "        pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "    )\n",
    "    assert pd.read_csv(Path(tmp_dir, \"scilint_warnings.csv\")).equals(\n",
    "        pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "    )\n",
    "    with open(Path(tmp_dir, \"scilint_config.json\")) as infile:\n",
    "        assert json.load(infile) == {\"out_dir\": tmp_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_load_conf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _load_conf(\n",
    "    conf_path: str = None,\n",
    "    exclusions: str = None,\n",
    "    fail_over: int = None,\n",
    "    out_dir: int = None,\n",
    "    precision: int = None,\n",
    "    print_syntax_errors: bool = None,\n",
    "):\n",
    "    if conf_path is None:\n",
    "        project_root = find_project_root(tuple([str(Path(\".\").resolve())]))\n",
    "        conf_path = Path(project_root, \"nbs\", \"scilint-default.yaml\")\n",
    "        print(f\"Loading default lint config: {conf_path}\")\n",
    "    else:\n",
    "        conf_path = Path(conf_path)\n",
    "\n",
    "    conf = yaml.safe_load(conf_path.read_text())\n",
    "    override_names = (\n",
    "        \"exclusions\",\n",
    "        \"fail_over\",\n",
    "        \"out_dir\",\n",
    "        \"precision\",\n",
    "        \"print_syntax_errors\",\n",
    "    )\n",
    "    overrides = (exclusions, fail_over, out_dir, precision, print_syntax_errors)\n",
    "    for override in zip(override_names, overrides):\n",
    "        if override[1] is not None:\n",
    "            conf[override[0]] = override[1]\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experimental_spec_path = Path(\n",
    "    Path(\".\"), \"example_nbs\", \"experimental\", \"scilint-experimental.yaml\"\n",
    ")\n",
    "experimental_spec = _load_conf(experimental_spec_path)\n",
    "assert experimental_spec[\"precision\"] == 3\n",
    "assert experimental_spec[\"fail_over\"] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experimental_spec = _load_conf(experimental_spec_path, fail_over=-1, precision=1)\n",
    "assert experimental_spec[\"precision\"] == 1\n",
    "assert experimental_spec[\"fail_over\"] == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `lint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def lint(\n",
    "    display_report: bool = True,\n",
    "    nb_glob: Path = None,\n",
    "    specs_glob: Path = Path(\".\").resolve(),\n",
    "    exclusions: str = None,\n",
    "    fail_over: int = None,\n",
    "    out_dir: int = None,\n",
    "    precision: int = None,\n",
    "    print_syntax_errors: bool = None,\n",
    "):\n",
    "    spec_nbs = _map_paths_specs(nb_glob, specs_glob)\n",
    "    lint_reports = []\n",
    "    all_warns = []\n",
    "    warns_count = []\n",
    "    for spec, nbs in spec_nbs.items():\n",
    "        conf = _load_conf(\n",
    "            spec, exclusions, fail_over, out_dir, precision, print_syntax_errors\n",
    "        )\n",
    "        if conf[\"evaluate\"] == False:\n",
    "            print(f\"Linting skipped for: {spec.name} as evaluate is set to false\")\n",
    "            continue\n",
    "        lint_report, report_warns, num_warnings = lint_nbs(\n",
    "            spec.name, conf, indicator_funcs, nb_paths=nbs\n",
    "        )\n",
    "        lint_reports.append(lint_report)\n",
    "        all_warns.append(report_warns)\n",
    "        warns_count.append(num_warnings)\n",
    "\n",
    "        fail_over_conf = conf[\"fail_over\"]\n",
    "        if conf[\"fail_over\"] == -1:\n",
    "            print(f\"Linting warnings ignored for: {spec.name} as fail_over set to -1\")\n",
    "        elif num_warnings == 0:\n",
    "            print(f\"Linting success for: {spec.name}, no issues found\")\n",
    "        elif num_warnings <= conf[\"fail_over\"]:\n",
    "            print(\n",
    "                f\"Linting success for: {spec.name}, warnings ({num_warnings}) <= than threshold ({fail_over_conf}) \"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"Linting failed for: {spec.name}, total warnings ({num_warnings}) exceeded threshold ({fail_over_conf})\"\n",
    "            )\n",
    "            sys.exit(num_warnings)\n",
    "\n",
    "    lint_report = pd.concat(lint_reports) if len(lint_reports) > 0 else lint_report\n",
    "    all_warns = pd.concat(all_warns) if len(all_warns) > 0 else report_warns\n",
    "    num_warnings = sum(warns_count)\n",
    "\n",
    "    if num_warnings > 0:\n",
    "        print(\n",
    "            f\"{num_warnings} warnings founds, within tolerated thresholds for all specs\"\n",
    "        )\n",
    "        if display_report:\n",
    "            display_warning_report(all_warns)\n",
    "    elif num_warnings == 0:\n",
    "        print(\"No issues found during linting\")\n",
    "\n",
    "    _persist_results(lint_report, all_warns, conf)\n",
    "    print(\"Linting completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linting success for: scilint-default.yaml, no issues found\n",
      "Linting success for: scilint-validated.yaml, no issues found\n",
      "Linting success for: scilint-experimental.yaml, warnings (3) <= than threshold (3) \n",
      "Linting success for: scilint-exploratory.yaml, warnings (1) <= than threshold (2) \n",
      "Linting skipped for: scilint-legacy.yaml as evaluate is set to false\n",
      "4 warnings founds, within tolerated thresholds for all specs\n",
      "\n",
      "******************************************Begin Scilint Warning Report*****************************************\n",
      "+---------------------------+--------------+------------------------------+---------+------------+-------------+\n",
      "| spec_name                 | notebook     | indicator                    |   value | operator   |   threshold |\n",
      "+===========================+==============+==============================+=========+============+=============+\n",
      "| scilint-experimental.yaml | non_nbdev    | asserts_func_ratio           |       0 | lt         |         1   |\n",
      "+---------------------------+--------------+------------------------------+---------+------------+-------------+\n",
      "| scilint-experimental.yaml | non_nbdev    | inline_asserts_per_func_mean |       0 | lt         |         0.5 |\n",
      "+---------------------------+--------------+------------------------------+---------+------------+-------------+\n",
      "| scilint-experimental.yaml | non_nbdev    | markdown_code_pct            |       0 | lt         |         5   |\n",
      "+---------------------------+--------------+------------------------------+---------+------------+-------------+\n",
      "| scilint-exploratory.yaml  | syntax_error | has_syntax_error             |       1 | equals     |         1   |\n",
      "+---------------------------+--------------+------------------------------+---------+------------+-------------+\n",
      "\n",
      "******************************************End Scilint Warning Report*******************************************\n",
      "\n",
      "Linting completed\n"
     ]
    }
   ],
   "source": [
    "lint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `build`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def build(\n",
    "    display_report: bool = True,\n",
    "    nb_glob: Path = None,\n",
    "    specs_glob: Path = Path(\".\").resolve(),\n",
    "    exclusions: str = None,\n",
    "    fail_over: int = None,\n",
    "    out_dir: int = None,\n",
    "    precision: int = None,\n",
    "    print_syntax_errors: bool = None,\n",
    "):\n",
    "    print(\"Tidying notebooks..\")\n",
    "    tidy()\n",
    "    if is_nbdev_project():\n",
    "        nbdev_export.__wrapped__()\n",
    "        print(\"Converted notebooks to modules\")\n",
    "        print(\"Testing notebooks..\")\n",
    "        nbdev_test.__wrapped__()\n",
    "    print(\"Running notebook linter..\")\n",
    "    lint(\n",
    "        display_report,\n",
    "        nb_glob,\n",
    "        specs_glob,\n",
    "        exclusions,\n",
    "        fail_over,\n",
    "        out_dir,\n",
    "        precision,\n",
    "        print_syntax_errors,\n",
    "    )\n",
    "    if is_nbdev_project():\n",
    "        nbdev_clean.__wrapped__()\n",
    "        print(\"Cleaned notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Console Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `scilint_tidy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def scilint_tidy():\n",
    "    tidy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `scilint_lint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def scilint_lint(\n",
    "    display_report: Param(\"Print the lint report\", store_false) = False,\n",
    "    nb_glob: Path = None,\n",
    "    specs_glob: Path = Path(\".\").resolve(),\n",
    "    exclusions: str = None,\n",
    "    fail_over: int = None,\n",
    "    out_dir: int = None,\n",
    "    precision: int = None,\n",
    "    print_syntax_errors: bool = None,\n",
    "):\n",
    "    lint(\n",
    "        display_report,\n",
    "        nb_glob,\n",
    "        specs_glob,\n",
    "        exclusions,\n",
    "        fail_over,\n",
    "        out_dir,\n",
    "        precision,\n",
    "        print_syntax_errors,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linting success for: scilint-default.yaml, no issues found\n",
      "Linting success for: scilint-validated.yaml, no issues found\n",
      "Linting success for: scilint-experimental.yaml, warnings (3) <= than threshold (3) \n",
      "Linting success for: scilint-exploratory.yaml, warnings (1) <= than threshold (2) \n",
      "Linting skipped for: scilint-legacy.yaml as evaluate is set to false\n",
      "4 warnings founds, within tolerated thresholds for all specs\n",
      "Linting completed\n"
     ]
    }
   ],
   "source": [
    "scilint_lint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `scilint_build`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def scilint_build(\n",
    "    display_report: Param(\"Print the lint report\", store_false) = False,\n",
    "    nb_glob: Path = None,\n",
    "    specs_glob: Path = Path(\".\").resolve(),\n",
    "    exclusions: str = None,\n",
    "    fail_over: int = None,\n",
    "    out_dir: int = None,\n",
    "    precision: int = None,\n",
    "    print_syntax_errors: bool = None,\n",
    "):\n",
    "    build(\n",
    "        display_report,\n",
    "        nb_glob,\n",
    "        specs_glob,\n",
    "        exclusions,\n",
    "        fail_over,\n",
    "        out_dir,\n",
    "        precision,\n",
    "        print_syntax_errors,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scilint_ci`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def scilint_ci(\n",
    "    display_report: Param(\"Print the lint report\", store_false) = False,\n",
    "    nb_glob: Path = None,\n",
    "    specs_glob: Path = Path(\".\").resolve(),\n",
    "    exclusions: str = None,\n",
    "    fail_over: int = None,\n",
    "    out_dir: int = None,\n",
    "    precision: int = None,\n",
    "    print_syntax_errors: bool = None,\n",
    "):\n",
    "    if not is_nbdev_project():\n",
    "        print(\"scilint_ci feature is only available for nbdev projects\")\n",
    "        return\n",
    "\n",
    "    build(\n",
    "        display_report,\n",
    "        nb_glob,\n",
    "        specs_glob,\n",
    "        exclusions,\n",
    "        fail_over,\n",
    "        out_dir,\n",
    "        precision,\n",
    "        print_syntax_errors,\n",
    "    )\n",
    "\n",
    "    if not shutil.which(\"quarto\"):\n",
    "        print(\n",
    "            \"Quarto is not installed. A working quarto install is required for the CI build\"\n",
    "        )\n",
    "        sys.exit(-1)\n",
    "    nbdev_readme.__wrapped__()\n",
    "    nbdev_docs.__wrapped__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
