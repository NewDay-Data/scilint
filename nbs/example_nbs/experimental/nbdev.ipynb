{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: test_nbdev.html\n",
    "title: nbdev format notebook Example\n",
    "\n",
    "skip_showdoc: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "# | default_exp experimental.test_nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def something():\n",
    "    message = \"The first step\"\n",
    "    print(f\"{message}\")\n",
    "    results = {\"message\": message}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = something()[\"message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-reloading modules is very useful when using `nbdev` as changes to underlying modules are picked up without having to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "\n",
    "> `sciflow` uses the papermill format for paramaeterising notebooks. See here for how to specify papermill params: https://papermill.readthedocs.io/en/latest/usage-parameterize.html. These parameters will be available to use in your flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "traffic_percent = 1\n",
    "workers = 8\n",
    "model_level = \"dispatcher\"\n",
    "min_date = \"2021-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_traffic_text(percent):\n",
    "    return str(percent) if int(percent) >= 10 else \"0\" + str(percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nbdev` tests are any cells which are not exporting code and do not have flags that say they should be ignored from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_traffic_text(\"3\") == \"03\"\n",
    "assert get_traffic_text(\"13\") == \"13\"\n",
    "assert get_traffic_text(\"78\") == \"78\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-exported helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_test_util(flag):\n",
    "    return not flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_experiment_segment(traffic_percent):\n",
    "    return tuple(get_traffic_text(tp) for tp in range(traffic_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_experiment_segment(1) == (\"00\",)\n",
    "assert get_experiment_segment(3) == (\"00\", \"01\", \"02\")\n",
    "assert \"' '\".join(get_experiment_segment(1)) == \"00\"\n",
    "assert f\"\"\"IN ('{\"','\".join(get_experiment_segment(3))}')\"\"\" == \"IN ('00','01','02')\"\n",
    "assert len(get_experiment_segment(50)) == 50\n",
    "assert max([int(x) for x in get_experiment_segment(100)]) == 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_utterances(model_level=None, min_date=None, traffic_percent=100):\n",
    "    \"\"\"\n",
    "    You will probably call data preparation code here. To simplify dependencies we are just creating synthetic data instead.\n",
    "    \"\"\"\n",
    "    get_experiment_segment(traffic_percent)\n",
    "    dummy_data = pd.Series(\n",
    "        np.random.choice(\n",
    "            [\n",
    "                \"Hello\",\n",
    "                \"Goodbye\",\n",
    "                \"Hi\",\n",
    "                \"Can you help?\",\n",
    "                \"I have an issue, can you help me?\",\n",
    "            ],\n",
    "            100,\n",
    "        ),\n",
    "        name=\"utterance\",\n",
    "    )\n",
    "    return dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def preprocess(message, model_level=None, min_date=None, traffic_percent=100):\n",
    "    print(f\"I captialised the message: {message.upper()}\")\n",
    "    data = get_utterances(model_level, min_date, traffic_percent)\n",
    "    documents = data.tolist()\n",
    "    results = {\"documents\": documents}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocess(message, traffic_percent)[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(documents) > 0\n",
    "assert (\n",
    "    pd.Series([\"Some other text\", \"Which should not be in the utterances\"])\n",
    "    .isin(pd.Series(documents))\n",
    "    .sum()\n",
    "    == 0\n",
    ")  # no button response texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Topics:\n",
    "    def __init__(self, documents, workers):\n",
    "        pass\n",
    "\n",
    "    def get_num_topics(self):\n",
    "        return 6\n",
    "\n",
    "    def get_topic_sizes(self):\n",
    "        return [1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    def get_topics(self, num_topics):\n",
    "        return (\n",
    "            [\"cat\", \"sat\", \"mat\", \"mouse\", \"house\", \"grouse\"],\n",
    "            np.asarray([1, 1, 1, 1, 1, 1]),\n",
    "            [1, 2, 3, 4, 5, 6],\n",
    "        )\n",
    "\n",
    "    def plot_wordcloud(self):\n",
    "        print(\"you may want to remove plotting code from testing to speed things up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def fit(documents, workers=workers):\n",
    "    model = Topics(documents, workers=workers)\n",
    "    results = {\"model\": model}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tests which are long running can be ignored from test execution. You can use the tst flags in settings.ini or create your own in the same file. See https://nbdev.fast.ai/test for more info. In this example we use `#slow` to indicate this should be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | slow\n",
    "import time\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit(documents, workers=workers)[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "assert all([s > 0 for s in topic_sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topic Words & Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())\n",
    "assert len(topic_words) == model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | vis\n",
    "# time.sleep(120)\n",
    "model.plot_wordcloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    topic_words, word_scores, topic_nums = model.get_topics(model.get_num_topics())\n",
    "\n",
    "    topic_contains_non_empty_words = all([len(tw) > 0 for tw in topic_words])\n",
    "    word_scores_in_range = word_scores.min() >= 0.0 and word_scores.max() <= 1.0\n",
    "    as_many_items_as_topics = (\n",
    "        model.get_num_topics() == len(topic_words) == word_scores.shape[0]\n",
    "    )\n",
    "    word_summaries = (\n",
    "        topic_contains_non_empty_words\n",
    "        and word_scores_in_range\n",
    "        and as_many_items_as_topics\n",
    "    )\n",
    "    # You can add artifacts in a step that will be saved to block storage. Add the paths to the file on the local filesystem\n",
    "    # and the artifact will be uploaded to remote storage.\n",
    "    sample_df = pd.DataFrame(\n",
    "        {\"a\": model.get_topic_sizes()[0], \"b\": model.get_topic_sizes()[1]}\n",
    "    )\n",
    "    sample_df.to_csv(\"/tmp/dataframe_artifact.csv\", index=False)\n",
    "    artifacts = [\"/tmp/dataframe_artifact.csv\"]\n",
    "    # You can add step metrics too this time just add a list of 3-tuples where tuple order = (name, value, step)\n",
    "    metrics = [(\"mae\", 100, 0), (\"mae\", 67, 1), (\"mae\", 32, 2)]\n",
    "    results = {\n",
    "        \"word_summaries\": word_summaries,\n",
    "        \"artifacts\": artifacts,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(model)\n",
    "assert results[\"word_summaries\"]\n",
    "assert results[\"metrics\"] == [(\"mae\", 100, 0), (\"mae\", 67, 1), (\"mae\", 32, 2)]\n",
    "assert results[\"artifacts\"] == [\"/tmp/dataframe_artifact.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def serve_num_topics(model):\n",
    "    return model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert serve_num_topics(model) > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:368653567616:image-version/sciflow/3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
