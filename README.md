# üßê `scilint`

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

`scilint` aims to **bring a style and quality standard into notebook
based Data Science workflows**. How you define a quality notebook is
difficult and somewhat subjective. It can have the obvious meaning of
being free of bugs but also legibility and ease of comprehension are
important too.

`scilint` takes the approach of breaking down potentially quality
relevant aspects of the notebook and providing what we believe are
sensible defaults that potentially correlate with higher quality
workflows. We also let users define the quality line as they see fit
through configuration of existing thresholds and ability to add new
metrics. As use of the library grows we anticipate being able to
statistically relate some of the quality relevant attributes to key
delivery metrics like ‚Äúchange failure rate‚Äù or ‚Äúlead time to
production‚Äù.

# Standing on the shoulders of giants - *an nbdev library*

> `scilint` is written on top of the excellent `nbdev` library. This
> library is revolutionary as it truly optimises all the benefits of
> notebooks and compensates for some of their weaker points. Converting
> your libraries to `nbdev` is not required for this tool to work but we
> argue that it would confer many benefits if you are part of a
> Production Data Science team. For more information on `nbdev` see the
> [homepage](https://nbdev.fast.ai/) or [github
> repo](https://github.com/fastai/nbdev)

# What is Notebook Quality?

This is a good quesiton and this library does not pretend to have the
answer. But we feel the problem space is worth exploring because the
value of high quality deliveries means lower time-to-market, less time
in re-work or rote porting of code and frees people up to think about
and solve hard problems.

That said, here is a very basic starting point where we would say that
**the notebook on left is of lower quality than the notebook on the
right**..

![Low vs High?](nbs/images/scilint_before_after_prep.png)

# Getting Started

`pip install scilint`

`scilint` has the following main features/commands:

1.  [`scilint_tidy`](https://newday-data.github.io/scilint/scilint.html#scilint_tidy):
    run an in-place opinionated flavour of
    [nbQA](https://github.com/nbQA-dev/nbQA) to tidy up your notebooks
2.  [`scilint_lint`](https://newday-data.github.io/scilint/scilint.html#scilint_lint):
    inspect the notebooks for potential quality correlates and report on
    the findings
3.  [`scilint_build`](https://newday-data.github.io/scilint/scilint.html#scilint_build):
    frequent usage build command to promote quality and consistency:
    {tidy, convert notebooks, test, lint, clean metadata}
4.  [`scilint_ci`](https://newday-data.github.io/scilint/scilint.html#scilint_ci):
    job designed to be used for continuous integration. Adds readme and
    docs generation to
    [`scilint_build`](https://newday-data.github.io/scilint/scilint.html#scilint_build).

## [`scilint_tidy`](https://newday-data.github.io/scilint/scilint.html#scilint_tidy)

To get a consistent style across your notebooks you can run
[`scilint_tidy`](https://newday-data.github.io/scilint/scilint.html#scilint_tidy);
this currently runs `autoflake`, `black` and `isort` in-place across all
of your notebooks.

## [`scilint_lint`](https://newday-data.github.io/scilint/scilint.html#scilint_lint)

Exposes potential quality issues within your notebook using some
pre-defined checks. Default threshold values for these checks are
provided that will enable a build to be marked as passed or failed.

## [`scilint_build`](https://newday-data.github.io/scilint/scilint.html#scilint_build)

Chains the following commands to ensure your project stays above the
quality and consistency bar:
[`scilint_tidy`](https://newday-data.github.io/scilint/scilint.html#scilint_tidy),
[nbdev_export](https://nbdev.fast.ai/api/export.html),
[nbdev_test](https://nbdev.fast.ai/api/test.html),
[`scilint_lint`](https://newday-data.github.io/scilint/scilint.html#scilint_lint),
[nbdev_clean](https://nbdev.fast.ai/api/clean.html)

## [`scilint_ci`](https://newday-data.github.io/scilint/scilint.html#scilint_ci)

Adds documentation generation to
[`scilint_build`](https://newday-data.github.io/scilint/scilint.html#scilint_build).
This requires a working quarto build.

### Potential Quality Indicators

The below are potential quality indicators that you can use to set a
minimum bar for quality and comprehensibility within your projects.
These are not exhaustive or definite quality indicators - they are a
starting point to open the conversation about what it means to have a
high quality notebook in practice. The indicators will evolve over time
with usage and feedback from user community.

- **Calls-Per-Function {Median, Mean}:** compares the amount of calls to
  the amount of functions. Looks for possible reletatioship between
  function definitions and usage.
- **In-Function-Percent:**: the percentage of code that is within a
  function rather than outside function scope.
- **Asserts-Per-Function:** looks at how many tests (where assert =
  test) there are compared to the total function count.
- **InlineAssertsPerFunction {Median, Mean}:** examines how many times a
  function is called within an assert statement - this aims to see
  testing of functions not just usage.
- **MarkdownToCodeRatio:** what is the ratio of markdown cells to code
  cells.
- **TotalCodeLen**: the total line length of the notebook.

Note: The following are a starting point for trying to reason about
notebook quality from some properties of the code and markdown within
them. There is no definitive answer as to whether any of these are low
or high quality. However there are reasons to believe inituitively that
higher or lower values of the below will produce higher quality
notebooks. There are many questions left ot answer, like the role of
docstrings and comments, their effectiveness may warrant inclusion but
that is an open quesiton at the moment. As this library is used and
refined with more projects and more experimental metrics then these
intuitions can evaluated more rigorously.

### What does a lint report look like?

![Sample Report](nbs/images/sample_report.png)

### Fail Threshold

> For now a very basic failure threshold is set by providing a number of
> warnings that will be accepted without failing the build. The default
> is 1 but this can be increased via the `--fail-over` parameter. As the
> library matures we will revisit adding more nuanced options.

# Contributing

After you clone this repository, please run nbdev_install_hooks in your
terminal. This sets up git hooks, which clean up the notebooks to remove
the extraneous stuff stored in the notebooks (e.g.¬†which cells you ran)
which causes unnecessary merge conflicts.

To run the tests in parallel, launch nbdev_test.

Before submitting a PR, check that the local library and notebooks
match.

If you made a change to the notebooks in one of the exported cells, you
can export it to the library with nbdev_prepare. If you made a change to
the library, you can export it back to the notebooks with nbdev_update.

# Appendix

`nbdev` contains many features that are useful for Data Science
workflows; too many in fact to cover here. We will focus on the major
features we consider to have significant for Production Data Science
teams:

1.  Explicit separation of exploration from what is **fundamental** for
    the workflow to execute using the `export` directive.
2.  Introducing a test runner for notebook-based repos.
3.  In-flow documentation of a notebook that is focused on the reader
    and powerfully expressive thanks to Quarto Markdown (aids building
    towards published reproducible research)
4.  Git friendly workflow via pre-commit hooks.
